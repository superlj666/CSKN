searchSpace:
  ### 1st stage
  # batch_size:
  #   _type: choice
  #   _value: [16, 32, 64, 128, 256]
  lr0:
    _type: choice    
    _value: [0.000001, 0.00001, 0.0001, 0.001]
  lr1:
    _type: choice
    _value: [0.000001, 0.00001, 0.0001, 0.001]
  # optimizer:
  #   _type: choice
  #   _value: ['SGD', 'Adadelta', 'Adagrad', 'Adam', 'Adamax']
  ### 2nd stage
  # kernel_par:
  #   _type: choice
  #   _value: [0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.00000001, 10.00000001]
  # kernel_par0:
  #   _type: choice
  #   _value: [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01]
  kernel_par1:
    _type: choice
    _value: [0.00000001, 0.0000001, 0.000001, 0.00001]
  growth_factor:
    _type: choice
    _value: [1, 2, 3, 4]
  # ### 3rd stage
  # lambda_0:
  #   _type: choice
  #   _value: [0.000000001, 0.00000001, 0.0000001, 0.000001, 0.00001]
  # lambda_1:
  #   _type: choice
  #   _value: [0.000000001, 0.00000001, 0.0000001, 0.000001, 0.00001]

trialCommand: python3 tune_hyperparameter.py
trialGpuNumber: 1
trialConcurrency: 4
maxTrialNumber: 200
tuner:
  name: TPE
  classArgs:
    # optimize_mode: minimize
    optimize_mode: maximize
trainingService:  # For other platforms, check mnist-pytorch example
  platform: local
  maxTrialNumberPerGpu: 2
  useActiveGpu: false  # NOTE: Use "true" if you are using an OS with graphical interface (e.g. Windows 10, Ubuntu desktop)
                       # Check the doc for details: https://nni.readthedocs.io/en/latest/reference/experiment_config.html#useactivegpu
